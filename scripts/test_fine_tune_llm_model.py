""" This script is used to test the fine-tuned larger LLMs models for ontology population from user text input files. """

import torch
import json
import os

from safetensors.torch import load_file

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    pipeline
)
from trl import setup_chat_format
from peft import PeftModel    

# get the path were the fine-tuned model is saved
checkpoint_dir = "/.../llama-3-70b-populate-ontology"


TUNED_MODEL_NAME = 'llama-3-70b-populate-ontology' 
TUNED_BASE_DIRECTORY = '{path to the fine-tuned model}'
tuned_model_directory = f'{TUNED_BASE_DIRECTORY}/{TUNED_MODEL_NAME}'

# path to the base model
MODEL_NAME = 'meta-llama/Llama-3.3-70B-Instruct'
BASE_DIRECTORY = './{path to the base model}'
base_model_directory = f'{BASE_DIRECTORY}/{MODEL_NAME}'

# 
adapters_name = tuned_model_directory

# attn_implementation = "eager" 
attn_implementation = "flash_attention_2" 


print(f"Starting to load the model {TUNED_MODEL_NAME} into memory")


# Reload tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(base_model_directory)

base_model_reload = AutoModelForCausalLM.from_pretrained(
    base_model_directory,
    return_dict=True,
    low_cpu_mem_usage=True,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
)

base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)

# merge adopter with base model
model = PeftModel.from_pretrained(
	base_model_reload, 
	checkpoint_dir
)

model = model.merge_and_unload()

# an example of user text to be translated into TTL graph
messages = [
    {
        "role": "system",
        "content": "Translate the user text into an TTL graph based on the TetraOnto ontology."
    },
    {
        "role": "user",
        "content": """The restoration project involves the creation of a fish pass at Brisach on the Rhine. This initiative is designed to improve the longitudinal continuity of the Rhine river, which is located in the Haut-Rhin department of France, in the commune of Brisach. The project owner is CERGA SA. This measure was made necessary by the construction of a micro-hydropower plant in 2008, a Franco-German infrastructure (EDF and EnBW) using a 5.7-meter head generated by an agricultural dam built between 1962 and 1965.
The works involved the creation of a fish pass with four entrances, a “classic” pass to facilitate upstream migration, and an downstream system with two structures installed on either side of the gate to enable fish to bypass the turbine. In addition, a trapping system has been installed for scientific fish monitoring.
Biological monitoring is carried out to assess the effectiveness of the restoration, although no hydromorphological monitoring has been undertaken. The project is expected to be completed by 2012 and has a total cost about 87,510 €."""
    }
]

prompt = tokenizer.apply_chat_template(messages, tokenize=False, 
                                       add_generation_prompt=True)

inputs = tokenizer(prompt, return_tensors='pt', padding=True, 
                   truncation=True).to("cuda")

outputs = model.generate(**inputs, max_length=2048, 
                         num_return_sequences=1)

text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(text.split("assistant")[1])

