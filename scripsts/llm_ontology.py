import torch
import json
import time
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import (
    LoraConfig,
    PeftModel,
    prepare_model_for_kbit_training,
    get_peft_model,
)
#import os, torch, wandb
#from datasets import load_dataset
from trl import SFTTrainer, setup_chat_format

MODEL_NAME = 'Qwen/Qwen2.5-72B-Instruct' #'meta-llama/Llama-3.3-70B-Instruct' #'meta-llama/Llama-3.2-3B-Instruct'
BASE_DIRECTORY = '/home2020/home/icube/fghazoua/TetraProject/models'
BNB_DIRECTORY = f'{BASE_DIRECTORY}/bnb'

model_directory = f'{BASE_DIRECTORY}/{MODEL_NAME}'
bnb_model_directory = f'{BNB_DIRECTORY}/{MODEL_NAME}'

torch_dtype = torch.float16
#attn_implementation = "eager" 
attn_implementation = "flash_attention_2" # need pip install -qqq flash-attn


from owlready2 import *
import json
from rdflib import Graph, Namespace
import PyPDF2

def loadontology(path_ontology):
    onto_path.clear()
    TetraWorld = World()
    onto1 = TetraWorld.get_ontology(path_ontology).load()



    # Save the ontology to a temporary file in RDF/XML format
    temp_file = "/home2020/home/icube/fghazoua/TetraProject/ontology/Tetra"
    onto1.save(file = temp_file, format = "rdfxml")

    # Read the content of the temporary file
    with open(temp_file, "r") as file:
        rdf_xml_content = file.read()
    
    # print(rdf_xml_content)
   
    # graph = default_world.as_rdflib_graph()
    graph = TetraWorld.as_rdflib_graph()

    TETRAONTO = Namespace("http://www.semanticweb.org/fethi/ontologies/2024/2/TetraOnto.owl#")
    # Bind the namespaces to their prefixes in the graph
    graph.bind("TetraOnto", TETRAONTO)

    ttl_file = "/home2020/home/icube/fghazoua/TetraProject/ontology/tetra.ttl" 
    graph.serialize(destination=ttl_file, format='turtle')

    with open(ttl_file, 'r') as file:
        ttl_data = file.read()
    return onto1, ttl_data, rdf_xml_content


tetraOnto_path = '/home2020/home/icube/fghazoua/TetraProject/ontology/TetraOntology_v2_without_instances.owl'
onto1, ttl_data, rdf_xml_content = loadontology(tetraOnto_path)

system = """can you print all the triples in the given turtle user ontology in this format:
<subject> <predicate> <object> 
You can use prefix TetraOnto to replace the iri 'http://www.semanticweb.org/fethi/ontologies/2024/1/TetraOnto.owl#' """

user = ttl_data

#system_prompt = f"""
#You are a helpful assistant who will use the :TetraOnto ontology that follows. This schema is called
#:TetraOnto with IRI <http://www.semanticweb.org/fethi/ontologies/2024/1/TetraOnto.owl#>

# Here a breif description of the gprovided ontology:\n\n
# {ontology_description}\n\n

# Make sure to give diffetent texts for different types of the restoration measure (see the ontology description)\n\n

#"""
user_prompt = """
I want to fine-tune an LLM on this TetraOnto ontology. 
This training aims to take an example user text and the assistant translates it into an TTL graph based on the TetraOnto ontology. 
Please create a comprehensive set of more than 40 examples system, user, and assistant messages in JSONL message conversational chat format for fine-tuning an LLM to translate text to TetraOnto ontology TTL.
The system content should be the instruction to translate the user text to the assistant TTL graph response using TetraOnto ontology.
The user text should be examples that cover all the concepts within the TetraOnto ontology (try to give a good text description, and gives examples of texts in either English or French). 
The assistant content should be an TTL graph (see the example below) using the TetraOnto ontology of the translation of the user text examples.
Use the prefix : with IRI <http://www.semanticweb.org/fethi/ontologies/2024/1/TetraOnto.owl#> for any created entities.
Use the message format as follows:
{"messages":[
{"role": "system", "content": "<system_content>"}, 
{"role": "user", "content": "<user_content>}, 
{"role": "assistant", "content": "<assistant_content>"}
]}\n

Here an example of your answer:\n
[\n
{"messages":[
{"role": "system", "content": "Translate the user text to a TTL graph using the TetraOnto ontology"},
{"role": "user", "content": "The restoration project involves the creation of a fish pass at Brisach on the Rhine. This initiative is designed to improve the longitudinal continuity of the Rhine river, which is located in the Haut-Rhin department of France, in the commune of Brisach. The project owner is CERGA SA. This measure was made necessary by the construction of a micro-hydropower plant in 2008, a Franco-German infrastructure (EDF and EnBW) using a 5.7-meter head generated by an agricultural dam built between 1962 and 1965.
The works involved the creation of a fish pass with four entrances, a “classic” pass to facilitate upstream migration, and an downstream system with two structures installed on either side of the gate to enable fish to bypass the turbine. In addition, a trapping system has been installed for scientific fish monitoring.
Biological monitoring is carried out to assess the effectiveness of the restoration, although no hydromorphological monitoring has been undertaken. The project is expected to be completed by 2012 and has a total cost about 87,510 €.
"},
{"role": "assistant", "content": "@prefix : <http://www.semanticweb.org/fethi/ontologies/2024/1/TetraOnto.owl#> .\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n@prefix skos: <http://www.w3.org/2004/02/skos/core#> .\n\n:PasseAPoissonsDeBrisach a :reconnection ;\n                            skos:prefLabel \"Passe à poissons de Brisach\" ;\n                            :hasMainContractor :CERGASA ;\n                            :isManagedBy :AssociationPassage309 ;\n                            :associatedTo :MicrocentraleHydroelectriqueDeBrisach ;\n                            :isLocatedOn :HautRhinvBrisachFrance ;\n                            :hasCategory :LongitudinalContinuity ;\n                            :includes :FishPassBrisach ,\n                                      :bypassSystemBrisach ,\n                                      :TrapSystemBrisach ;\n                            :hasNumberOfEntries 4 ;\n                            :hasCost "87,510€" ;\n                            :hasBiologicalMonitoring \"true\"^^xsd:boolean ;\n                            :hasHydromorphologicalMonitoring \"false\"^^xsd:boolean ;\n                            :startsAt \"2008-01-01\"^^xsd:dateTime ;\n                            :endsAt \"2012-12-30\"^^xsd:dateTime .\n\n:MicrocentraleHydroelectriqueDeBrisach a :dam ;\n                                        skos:prefLabel \"Microcentrale Hydroélectrique de Brisach\" ;\n                                        :isLocatedOn :LeRhin ,\n			                             :HautRhinvBrisachFrance ;\n                                        :hasHeight \"5.7\"^^xsd:decimal ;\n                                        :isImpassable \"true\"^^xsd:boolean .\n\n:LeRhin a :river ;\n        skos:prefLabel \"Le Rhin\" .\n\n:CERGASA a :mainContractor ;\n          skos:prefLabel \"CERGA SA\" .\n\n:AssociationPassage309 a :projectOwner ;\n          skos:prefLabel \"CERGA SA\" .\n\n:HautRhinBrisachFrance a :geographicZone ;\n                          skos:prefLabel \"Brisach, Haut-Rhin, France\" .\n\n:FishPassBrisach a :fishway;\n                skos:prefLabel \"passe à poisson de Brisach\" .\n\n:bypassSystemBrisach a :bypassSystem;\n			:hasFunction \"enable fish to bypass the turbine\" ;\n                        skos:prefLabel \"bypass System Brisach\" .\n\n:TrapSystemBrisach a :trapSystem;\n			:hasFunction \"scientific fish monitoring\" ;\n                        skos:prefLabel \"Trap System Brisach\" ."}\n]}\n]

Instructions note:\n
     - do not repeat the given example.
     - generated examples should be distinct and can be written in either English or French.
     - generated examples include different water body (river,...) from the world.
     - generated examples should complete and include information about 'restoration project measure', 'structure' ("dam", "weir", "nozzle", "pondDyke", "gate"), 'water body', 'geographic zone', 'project manager', and so on. (see the ontology classes)
     - changes the sentence structure of the user text every time. 
     - for each generated example, the main class should be one of the concept of 'Restoration project measure' ("reconnection", "poolCreation", "de-silting", "regularFlooding", "floodRetention", "poolRestoration", "channelCreation", "riverbankRestoration", "routeModification").
     - make shure that all object and data properties are well defined as in the given example.
"""


# QLoRA config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch_dtype,
    bnb_4bit_use_double_quant=True,
)

print("Process started....")
start_time = time.time()
# Load model
model = AutoModelForCausalLM.from_pretrained(
    model_directory,
    quantization_config=bnb_config,
    device_map="auto",
    attn_implementation=attn_implementation
)


# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_directory)

if hasattr(tokenizer, "chat_template") and tokenizer.chat_template is not None:
    tokenizer.chat_template = None  # Reset the chat template

model, tokenizer = setup_chat_format(model, tokenizer)

messages = [
    
            {"role": "system", "content": system},
            {"role": "user", "content": user}
]

prompt = tokenizer.apply_chat_template(messages, tokenize=False, 
                                       add_generation_prompt=True)

inputs = tokenizer(prompt, return_tensors='pt', padding=True, 
                   truncation=True).to("cuda")

outputs = model.generate(**inputs, max_length=28000, 
                         num_return_sequences=1)
# max_length=16200 , 30000

text = tokenizer.decode(outputs[0], skip_special_tokens=True)

end_time = time.time()
execution_time = end_time - start_time
minutes, seconds = divmod(execution_time, 60)
formatted_time = f"{int(minutes)} minutes and {seconds:.2f} seconds"
print(f"Execution time for model '{MODEL_NAME}' is: {formatted_time}")

print(text)

# Enregistrer les résultats dans un fichier JSON
# with open("/home2020/home/icube/fghazoua/TetraProject/dataset_Llama3_7.json", "w") as json_file:
#     json.dump(text, json_file, indent=4)

