import torch
import json
import os
import time

from safetensors.torch import load_file

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    pipeline
    # LlamaForCausalLM,
    # BitsAndBytesConfig,
    # LlamaTokenizer
)
from trl import setup_chat_format
from peft import PeftModel    

# checkpoint_dir = "/home2020/home/icube/fghazoua/TetraProject/scripts/Qwen2.5-72B-Instruct-populate-ontology"
checkpoint_dir = "/home2020/home/icube/fghazoua/TetraProject/scripts/llama-3-70b-populate-ontology"

#sd = load_file("/home2020/home/icube/fghazoua/TetraProject/scripts/llama-3-70b-populate-ontology/checkpoint-64/adapter_model.safetensors")
#for key, val in sd.items():
#    print(key, val.shape)

TUNED_MODEL_NAME = 'llama-3-70b-populate-ontology' 
TUNED_BASE_DIRECTORY = '/home2020/home/icube/fghazoua/TetraProject/scripts'

# TUNED_MODEL_NAME = 'Qwen2.5-72B-Instruct-populate-ontology' 
# TUNED_BASE_DIRECTORY = '/home2020/home/icube/fghazoua/TetraProject/scripts'
tuned_model_directory = f'{TUNED_BASE_DIRECTORY}/{TUNED_MODEL_NAME}'

# MODEL_NAME = 'Qwen/Qwen2.5-72B-Instruct' #'meta-llama/Llama-3.3-70B-Instruct' #'meta-llama/Llama-3.2-3B-Instruct'
MODEL_NAME = 'meta-llama/Llama-3.3-70B-Instruct'
BASE_DIRECTORY = '/home2020/home/icube/fghazoua/TetraProject/models'
base_model_directory = f'{BASE_DIRECTORY}/{MODEL_NAME}'

# 
adapters_name = tuned_model_directory

# attn_implementation = "eager" 
attn_implementation = "flash_attention_2" 

# QLoRA config
#bnb_config = BitsAndBytesConfig(
#    load_in_4bit=True,
#    bnb_4bit_quant_type="nf4",
#    bnb_4bit_compute_dtype=torch.float16,
#    bnb_4bit_use_double_quant=True,
#)

print("Process started....")
start_time = time.time()

print(f"Starting to load the model {TUNED_MODEL_NAME} into memory")


# Reload tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(base_model_directory)

base_model_reload = AutoModelForCausalLM.from_pretrained(
    base_model_directory,
    return_dict=True,
    low_cpu_mem_usage=True,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
    # quantization_config=bnb_config,
    # device_map='auto',
    # attn_implementation=attn_implementation
)

base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)

# merge adopter with base model
model = PeftModel.from_pretrained(
	base_model_reload, 
	checkpoint_dir
	# model_id=checkpoint_dir,
	#peft_config=bnb_config,
	#device_map='auto',
	# attn_implementation=attn_implementation
)

model = model.merge_and_unload()

# Load tokenizer
# tokenizer = LlamaTokenizer.from_pretrained(checkpoint_dir)

# Load tokenizer
# tokenizer = AutoTokenizer.from_pretrained(model_directory)

# if hasattr(tokenizer, "chat_template") and tokenizer.chat_template is not None:
#     tokenizer.chat_template = None  # Reset the chat template

# model, tokenizer = setup_chat_format(model, tokenizer)

# testing the new model

messages = [
    {
        "role": "system",
        "content": "Translate the user text into an TTL graph based on the TetraOnto ontology."
    },
    {
        "role": "user",
        "content": """The restoration project involves the creation of a fish pass at Brisach on the Rhine. This initiative is designed to improve the longitudinal continuity of the Rhine river, which is located in the Haut-Rhin department of France, in the commune of Brisach. The project owner is CERGA SA. This measure was made necessary by the construction of a micro-hydropower plant in 2008, a Franco-German infrastructure (EDF and EnBW) using a 5.7-meter head generated by an agricultural dam built between 1962 and 1965.
The works involved the creation of a fish pass with four entrances, a “classic” pass to facilitate upstream migration, and an downstream system with two structures installed on either side of the gate to enable fish to bypass the turbine. In addition, a trapping system has been installed for scientific fish monitoring.
Biological monitoring is carried out to assess the effectiveness of the restoration, although no hydromorphological monitoring has been undertaken. The project is expected to be completed by 2012 and has a total cost about 87,510 €."""
    }
]

prompt = tokenizer.apply_chat_template(messages, tokenize=False, 
                                       add_generation_prompt=True)

inputs = tokenizer(prompt, return_tensors='pt', padding=True, 
                   truncation=True).to("cuda")

outputs = model.generate(**inputs, max_length=2048, 
                         num_return_sequences=1)

text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(text.split("assistant")[1])

model.save_pretrained("/home2020/home/icube/fghazoua/TetraProject/llama-3-70b-populate-ontology_2")
tokenizer.save_pretrained("/home2020/home/icube/fghazoua/TetraProject/llama-3-70b-populate-ontology_2")

end_time = time.time()
execution_time = end_time - start_time
minutes, seconds = divmod(execution_time, 60)
formatted_time = f"{int(minutes)} minutes and {seconds:.2f} seconds"
print(f"Execution time for model '{MODEL_NAME}' is: {formatted_time}")
