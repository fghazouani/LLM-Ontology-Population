{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_text_file(file_path):\n",
    "    \"\"\"Reads text content from a file.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "def split_text_into_chunks(text, max_chunk_size):\n",
    "    \"\"\"Splits text into chunks based on the maximum token size.\"\"\"\n",
    "    return [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontology description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_description = f\"\"\"\n",
    "The ontology prefix: TetraOnto: <http://www.semanticweb.org/fethi/ontologies/2024/1/TetraOnto.owl#>\\n\\n \n",
    "Classes :\n",
    "    - \"geographicZone\",\n",
    "    - \"mainContractor\", \"projectOwner\", \"otherActor\",\n",
    "    - Migratory species classes:\n",
    "        \"amphibioticMigrant\", \n",
    "   \n",
    "    - Fishway types:\n",
    "        \"basinPasses\", \"successivesBasinsPass\", \"preBarrage\", \"speedBumpsPass\", \n",
    "        \"fishLift\", \"fishLock, \"artificialRiver\", \"verticalSlotPassage\", \"baffleFishway\",\n",
    "        \"eelLadder\", \"trapPass\", \"studSubstrate\", \"baffleBrush\", \n",
    "    \n",
    "    - Structure types: \n",
    "        '\"dam\", \"weir\", \"nozzle\", \"pondDyke\", \"gate\",\n",
    "    \n",
    "    - Water body types: \n",
    "        \"affluent\", \"arm\", \"counterDrainageChannel\", \"pool\", \"river\", \n",
    "    \n",
    "    - Restoration measure types: \n",
    "        \"reconnection\", \"poolCreation\", \"de-silting\", \"regularFlooding\",\n",
    "        \"floodRetention\", \"poolRestoration\", \"riverbankRestoration\", \"channelCreation\"\\n\n",
    "\n",
    "  Objects Properties:\n",
    "    - \"relizedIn\", means that a \"restoration measure\" is relizedIn a \"geographic zone\",\n",
    "    - \"associatedTo\", means that a \"restoration measure\" is associatedTo a \"structure\"\n",
    "    - \"isLocatedOn\", means that a \"structure\" isLocatedOn a \"water body\"\n",
    "    - \"isManagedBy\", means that a project of \"restoration measure\" isManagedBy by \"project \"owner\" and/or by \"other actor\"\n",
    "    - \"containsTechnicalElement\", means that a \"restoration measure\" contains a \"technical element\"\n",
    "    - \"hasMainContractor\",\n",
    "    - \"concerns\", means that a \"restoration measure\" belongs to a \"type of restoration\"\n",
    "\n",
    "  Data Properties:\n",
    "    - \"hasCost\": a project of restoration measure has some cost\n",
    "    - \"startsAt\": a restoration measure starts at a date time\n",
    "    - \"endssAt\": a restoration measure starts at a date time\n",
    "    - \"hasHeight\": a \"structure\" hasHeight \"decimal\"\n",
    "    - \"isImpassable\": to say if a \"structure\" is considered impassable (true) if its height is greater than 1.0 m)\n",
    "    - \"hasMaxLowWaterFlow\": a river has Max Low Water Flow\n",
    "    - \"hasMixLowWaterFlow\": a river has Min Low Water Flow\n",
    "    - \"hasPoolLength\": the pool length\n",
    "    - \"hasPoolWidth\": the pool width  \n",
    "    - \"hasNotchHight\": the noch hight \n",
    "    - \"hasNotchWidth\": the noch width \n",
    "    - \"numberOfPools\": e.g., a river or artificial river instance has a number of pools\n",
    "    - \"numberOfBasins\": the number of basins\n",
    "    - \"averageSlope\": a river or artificial river instance has a number of average slope\n",
    "    - \"hasBiologicalMonitoring\", a restoration mesure is subject or no to Biological Monitoring\n",
    "    - \"hasHydromorphologicaMonitoring\", a restoration mesure is subject or no to Hydromorphologica Monitoring\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input text files & LLMs list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing input text files\n",
    "folder_path = \"/.../base_test\"  \n",
    "\n",
    "# List of models to apply\n",
    "models = [\"qwen2.5:14b\",\"qwen2:7b\", \"llama3.2:latest\", \"llama3.1:8b\", \"llama3:latest\", \"mistral:latest\"]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot without template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_folder = \"/.../zero_shot_without_Template_outputs\" \n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "system_content = f\"\"\"You are provided with an ontology description and a text input. Create instances (individuals) extracted from the text based on this ontology.\n",
    "The extracted indivuduals output will be in turtle format.\n",
    "Do not include any additional information\\n\n",
    "\n",
    "Here is the current ontology description:\n",
    "{ontology_description}\\n\\n\n",
    "\n",
    "Text input:\\n\n",
    "\"\"\"\n",
    "\n",
    "# Process each text file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Skip non-text files\n",
    "    if not file_name.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    # Read the content of the file\n",
    "    user_content = read_text_file(file_path)\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"Processing file '{file_name}' with model '{model}'...\")\n",
    "\n",
    "        # Start timing the execution\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=model,  \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Stop timing the execution\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate execution time\n",
    "        execution_time = end_time - start_time\n",
    "        execution_times[(file_name, model)] = execution_time\n",
    "\n",
    "        minutes, seconds = divmod(execution_time, 60)\n",
    "        formatted_time = f\"{int(minutes)} minutes and {seconds:.2f} seconds\"\n",
    "        \n",
    "        print(f\"Execution time for model '{model}' on file '{file_name}': {formatted_time}\")\n",
    "\n",
    "        output_file_name = f\"{os.path.splitext(file_name)[0]}_{model.replace(':', '_')}.ttl\"\n",
    "        output_file_path = os.path.join(output_folder, output_file_name)\n",
    "\n",
    "        # LLM response content\n",
    "        turtle_data = response[\"message\"]['content']\n",
    "\n",
    "        with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file: \n",
    "            output_file.write(\"@prefix : <http://www.semanticweb.org/fethi/ontologies/2024/1/TetraOnto.owl#> .\\n\")\n",
    "            output_file.write(\"@prefix skos: <http://www.w3.org/2004/02/skos/core#> .\\n\")\n",
    "            output_file.write(\"@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\\n\")\n",
    "            output_file.write(turtle_data)\n",
    "\n",
    "print(\"Processing complete for all files and models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot with template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = read_text_file('/.../template.txt')\n",
    "\n",
    "output_folder = \"/.../zero_shot_template_output\"  \n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "system_content = f\"\"\"You are provided with an ontology description and a text input. Create instances (individuals) extracted from the text based on the ontology description and the provided following template:\n",
    "Do not include any additional information\\n\n",
    "\n",
    "Here is the current ontology description:\n",
    "{ontology_description}\\n\\n\n",
    "\n",
    "Here is the given template to follow:\\n\n",
    "{template}\\n\\n\n",
    "\n",
    "Text input:\\n\n",
    "\"\"\"\n",
    "\n",
    "# Process each text file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Skip non-text files\n",
    "    if not file_name.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    # Read the content of the file\n",
    "    user_content = read_text_file(file_path)\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"Processing file '{file_name}' with model '{model}'...\")\n",
    "\n",
    "        # Start timing the execution\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=model,  \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Stop timing the execution\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate execution time\n",
    "        execution_time = end_time - start_time\n",
    "        execution_times[(file_name, model)] = execution_time\n",
    "\n",
    "        minutes, seconds = divmod(execution_time, 60)\n",
    "        formatted_time = f\"{int(minutes)} minutes and {seconds:.2f} seconds\"\n",
    "        \n",
    "        print(f\"Execution time for model '{model}' on file '{file_name}': {formatted_time}\")\n",
    "\n",
    "        output_file_name = f\"{os.path.splitext(file_name)[0]}_{model.replace(':', '_')}.ttl\"\n",
    "        output_file_path = os.path.join(output_folder, output_file_name)\n",
    "\n",
    "        # LLM response content\n",
    "        turtle_data = response[\"message\"]['content']\n",
    "\n",
    "        with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file: \n",
    "            output_file.write(\"@prefix : <http://www.semanticweb.org/fethi/ontologies/2024/1/TetraOnto.owl#> .\\n\")\n",
    "            output_file.write(\"@prefix skos: <http://www.w3.org/2004/02/skos/core#> .\\n\")\n",
    "            output_file.write(\"@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\\n\")\n",
    "            output_file.write(turtle_data)\n",
    "\n",
    "print(\"Processing complete for all files and models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_response_with_context(chunks, model, ontology_summary, assistant_message):\n",
    "    # Initialise le contexte avec un résumé de l'ontologie\n",
    "    context = ontology_summary\n",
    "    responses = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        prompt = f\"\\n\\nTexte :\\n{chunk}\\n\\n\"\n",
    "\n",
    "        # Appel du modèle avec le prompt\n",
    "        response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": ontology_summary},\n",
    "            {\"role\": \"user\", \"content\": chunk}\n",
    "        ]\n",
    "    )\n",
    "        responses.append(response['message']['content'])\n",
    "        context += f\"Réponse pour le chunk {i+1} : {response['message']['content']}\\n\"\n",
    "    \n",
    "    return responses\n",
    "\n",
    "system_message = f\"\"\"\n",
    "You are an experienced instance extractor. \n",
    "Extract all instances from the following text and map them to the appropriate entities on the provided ontology description.\\n\n",
    "Do not include any additional information.\\n\n",
    "\n",
    "Here is the ontology description:\\n\n",
    "{ontology_description}\\n\\n\n",
    "For each instance, write the result as follows:\n",
    "\n",
    "    IndividualName is a :<class>\n",
    "        :<ObjectProperty> to <AnotherIndividualName>\n",
    "        :<DataProperty> : PropertyValue \n",
    "        label: \"Individual Name\"\n",
    "        \n",
    "    <AnotherIndividualName> is a :<class>\n",
    "        label : \"Another Individual Name\" \\n\n",
    "\n",
    "Create the relationship between <IndividualName> and <AnotherIndividualName>.\\n\n",
    "\n",
    "Example Text: \n",
    "Mesure de restauration : Passe à poissons de Strasbourg\n",
    "Masse(s) d'eau: Le Rhin\n",
    "Catégorie: Continuité longitudinale\n",
    "Pays: France\n",
    "Département/Land: Bas-Rhin\n",
    "Commune(s): Strasbourg\n",
    "Maître d'oeuvre: Centre d'Ingénierie Hydraulique d'EDF, EDF R&D\n",
    "Maître d'ouvrage: Electricité De France (EDF) Unité de production est\n",
    "Autres acteurs: Agence de l'eau Rhin-Meuse, DREAL Alsace, ONEMA\n",
    "Début des travaux: 2012\n",
    "Fin des travaux: Prévue en 2015\n",
    "Pression(s) sur la zone: Le barrage de la centrale hydroélectrique est l’un des obstacles d’une série. Il perturbe la remonté du Rhin vers les zones favorables à la vie et à la reproduction des poissons. \n",
    "Travaux et aménagements: Création de la passe à poissons, avec :\n",
    "   • Une succession de 18 bassins en bétons (hauteur entre bassins : 20cm)\n",
    "Suivi biologique: oui\n",
    "Suivi hydromorphologique: non\n",
    "Coût total de l'opération: 15000000.0\n",
    "\n",
    "Answer:\\n\n",
    "PasseAPoissonsDeStrasbourg a :successivesBasinsPass \n",
    "    :associatedTo BarrageCentraleHydroélectrique\n",
    "    :isManagedBy EDFUnitéDeProductionEst\n",
    "    :hasMainContractor CentreIngénierieHydrauliqueEDFandEDF_R&D\n",
    "    :numberOfBasins \"18\"^^xsd:integer \n",
    "    :startsAt \"2012\"^^xsd:dateTime \n",
    "    :endsAt \"2015\"^^xsd:dateTime\n",
    "    :hasCost \"15000000.0\"^^xsd:decimal \n",
    "    :hasBiologicalMonitoring \"true\"^^xsd:boolean\n",
    "    :hasHydromorphologicalMonitoring \"false\"^^xsd:boolean\n",
    "    :concerns :LongitudinalContinuity \n",
    "\n",
    "BarrageCentraleHydroélectrique is a :dam \n",
    "    :isLocatedOn BasRhinStrasbourgFrance\n",
    "    :isLocatedOn LeRhin\n",
    "    :isImpassable \"true\"^^xsd:boolean\n",
    "    label : \"Barrage Centrale Hydroélectrique\"\n",
    "\n",
    "EDFUnitéDeProductionEst is a :mainContractor\n",
    "    label : \"EDF Unité de production est\"\n",
    "\n",
    "LeRhin is a :river \n",
    "    :isLocatedOn DépartementduBasRhin\n",
    "    label : \"Le Rhin\"\n",
    "\n",
    "CentreIngénierieHydrauliqueEDFandEDF_R&D is a :projectOwner \n",
    "    label : \"Centre Ingénierie Hydraulique EDF, EDF R&D\"\n",
    "\n",
    "AgenceEauRhin-Meuse_DREALAlsace_ONEMA is a :otherActor \n",
    "    label : \"Agence de l'eau Rhin-Meuse, DREAL Alsace, ONEMA\"\n",
    "\n",
    "BasRhinStrasbourgFrance is a :geographicZone\n",
    "    label : \"Bas-Rhin, Strasbourg, France\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Parameters\n",
    "few_shot_examples = read_text_file('few_shot_3.txt')\n",
    "assistant_message = few_shot_examples\n",
    "\n",
    "ontology_summary = system_message\n",
    "\n",
    "max_chunk_size = 3000  \n",
    "\n",
    "# Folder to save outputs\n",
    "output_folder = \"/.../one_shot_outputs\"  \n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Process each text file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Skip non-text files\n",
    "    if not file_name.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    # Read the content of the file\n",
    "    user_content = read_text_file(file_path)\n",
    "\n",
    "    # Split the text into manageable chunks\n",
    "    chunks = split_text_into_chunks(user_content, max_chunk_size)\n",
    "\n",
    "    # Apply each model to the chunks\n",
    "    for model in models:\n",
    "        print(f\"Processing file '{file_name}' with model '{model}'...\")\n",
    "\n",
    "        # Start timing the execution\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Generate responses for all chunks using the current model\n",
    "        responses = generate_response_with_context(chunks, model, ontology_summary, assistant_message)\n",
    "\n",
    "        # Save the responses to an output file\n",
    "        output_file_name = f\"{os.path.splitext(file_name)[0]}_{model.replace(':', '_')}.txt\"\n",
    "        output_file_path = os.path.join(output_folder, output_file_name)\n",
    "\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "            for i, response in enumerate(responses):\n",
    "                output_file.write(f\"Response for chunk {i + 1}:\\n\\n{response}\\n\\n\")\n",
    "        \n",
    "        individuals = read_text_file(output_file_path)\n",
    "\n",
    "\n",
    "        prompt2 = f\"\"\"Follow the format of generated individuals below to translate the following input instances\\n\n",
    "        Do not include any additional information\\n\n",
    "        Be sure to link all the generated individuals (no isolated individual)\\n\n",
    "\n",
    "        # Example of generated individuals:\\n\n",
    "        {assistant_message}\\n\\n\n",
    "\n",
    "        # Input Instances to be translated:\\n\\n\n",
    "        \"\"\"\n",
    "        response2 = ollama.chat(model=model, messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt2},\n",
    "            {\"role\": \"user\", \"content\": individuals}\n",
    "        ])\n",
    "\n",
    "\n",
    "        # Stop timing the execution\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate execution time\n",
    "        execution_time = end_time - start_time\n",
    "        execution_times[(file_name, model)] = execution_time\n",
    "\n",
    "        minutes, seconds = divmod(execution_time, 60)\n",
    "        formatted_time = f\"{int(minutes)} minutes and {seconds:.2f} seconds\"\n",
    "        \n",
    "        print(f\"Execution time for model '{model}' on file '{file_name}': {formatted_time}\")\n",
    "\n",
    "        output_file_name = f\"{os.path.splitext(file_name)[0]}_{model.replace(':', '_')}.ttl\"\n",
    "        output_file_path = os.path.join(output_folder, output_file_name)\n",
    "\n",
    "        # LLM response content\n",
    "        turtle_data = response2[\"message\"]['content']\n",
    "\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file: \n",
    "            output_file.write(turtle_data)\n",
    "\n",
    "        print(f\"Saved responses to '{output_file_path}'.\")\n",
    "\n",
    "print(\"Processing complete for all files and models.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
